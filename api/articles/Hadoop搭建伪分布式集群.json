{"title":"Hadoop搭建伪分布式集群","uid":"bb0b195ced0db413665f5bd6467e3504","slug":"Hadoop搭建伪分布式集群","date":"2022-05-30T06:07:49.000Z","updated":"2022-05-30T07:50:35.546Z","comments":true,"path":"api/articles/Hadoop搭建伪分布式集群.json","keywords":null,"cover":"/post/Hadoop搭建伪分布式集群/cover.jpeg","content":"<hr>\n<p>Hadoop伪分布式集群即在一台Linux机器上部署Hadoop集群，通过开通不同的端口来实现相应集群搭建。</p>\n<hr>\n<h3 id=\"Hadoop伪分布式集群安装\"><a href=\"#Hadoop伪分布式集群安装\" class=\"headerlink\" title=\"Hadoop伪分布式集群安装\"></a>Hadoop伪分布式集群安装</h3><p><img src=\"/post/Hadoop%E6%90%AD%E5%BB%BA%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%9B%86%E7%BE%A4/install-1.png\" alt=\"Hadoop\"></p>\n<p>这张图代表是一台Linux机器，也可以称为是一个节点，上面安装的有JDK环境</p>\n<p>最上面的是Hadoop集群会启动的进程，其中NameNode、SecondaryNameNode、DataNode是HDFS服务的进程，ResourceManager、NodeManager是YARN服务的进程，MapRedcue在这里没有进程，因为它是一个计算框架，等Hadoop集群安装好了以后MapReduce程序可以在上面执行。</p>\n<h4 id=\"配置基础环境\"><a href=\"#配置基础环境\" class=\"headerlink\" title=\"配置基础环境\"></a>配置基础环境</h4><blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>静态ip配置、hostname配置、firewalld关闭、ssh免密登录、JDK</p></blockquote>\n<h4 id=\"安装Hadoop\"><a href=\"#安装Hadoop\" class=\"headerlink\" title=\"安装Hadoop\"></a>安装Hadoop</h4><ol>\n<li><p>首先把下载好的hadoop安装包上传到/opt/software文件夹下</p>\n<pre class=\"line-numbers language-shell\" data-language=\"shell\"><code class=\"language-shell\">[root@bigdata-01 software]# ll\n总用量 524788\n-rw-r--r--. 1 root root 345625475 5月  30 14:15 hadoop-3.2.0.tar.gz\n-rwxr-xr-x. 1 root root 191753373 1月  29 2021 jdk-8u191-linux-x64.tar.gz</code></pre></li>\n<li><p>解压Hadoop压缩包</p>\n<pre class=\"line-numbers language-shell\" data-language=\"shell\"><code class=\"language-shell\">tar -zxvf hadoop-3.2.0.tar.gz -C &#x2F;opt&#x2F;moudle&#x2F;\n[root@bigdata-01 software]# cd &#x2F;opt&#x2F;moudle&#x2F;\n[root@bigdata-01 moudle]# ll\n总用量 0\ndrwxr-xr-x. 9 1001 1002 149 1月   8 2019 hadoop-3.2.0\ndrwxr-xr-x. 7   10  143 245 10月  6 2018 jdk1.8.0_191</code></pre></li>\n<li><p>修改Hadoop的配置文件</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>主要修改以下文件：</p>\n<p>hadoop-env.sh</p>\n<p>core-site.xml</p>\n<p>hdfs-site.xml</p>\n<p>mapred-site.xml</p>\n<p>yarn-site.xml workers</p></blockquote>\n<ul>\n<li><p>首先修改 hadoop-env.sh 文件，增加环境变量信息，添加到hadoop-env.sh 文件末尾即可。</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>JAVA_HOME：指定java的安装位置</p>\n<p>HADOOP_LOG_DIR：hadoop的日志的存放目录</p></blockquote>\n<pre class=\"line-numbers language-shell\" data-language=\"shell\"><code class=\"language-shell\">[root@bigdata-01 hadoop]# vim hadoop-env.sh\n# For example, to limit who can execute the namenode command,\n# export HDFS_NAMENODE_USER&#x3D;hdfs\n\nexport JAVA_HOME&#x3D;&#x2F;opt&#x2F;moudle&#x2F;jdk1.8.0_191\nexport HADOOP_LOG_DIR&#x3D;&#x2F;data&#x2F;hadoop_repo&#x2F;logs&#x2F;hadoop</code></pre></li>\n<li><p>修改core-site.xml 文件</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>注意 fs.defaultFS 属性中的主机名需要和你配置的主机名保持一致</p></blockquote>\n<pre class=\"line-numbers language-shell\" data-language=\"shell\"><code class=\"language-shell\">[root@bigdata-01 hadoop]# vim core-site.xml \n&lt;!-- Put site-specific property overrides in this file. --&gt;\n&lt;configuration&gt;\n    &lt;property&gt;\n        &lt;name&gt;fs.defaultFS&lt;&#x2F;name&gt;\n        &lt;value&gt;hdfs:&#x2F;&#x2F;bigdata-01:9000&lt;&#x2F;value&gt;\n    &lt;&#x2F;property&gt;\n    &lt;property&gt;\n        &lt;name&gt;hadoop.tmp.dir&lt;&#x2F;name&gt;\n        &lt;value&gt;&#x2F;data&#x2F;hadoop_repo&lt;&#x2F;value&gt;\n   &lt;&#x2F;property&gt;\n&lt;&#x2F;configuration&gt;</code></pre></li>\n<li><p>修改hdfs-site.xml文件，把hdfs中文件副本的数量设置为1，因为现在伪分布集群只有一个节点</p>\n<pre class=\"line-numbers language-shell\" data-language=\"shell\"><code class=\"language-shell\">[root@bigdata-01 hadoop]# vim hdfs-site.xml \n&lt;configuration&gt;\n    &lt;property&gt;\n        &lt;name&gt;dfs.replication&lt;&#x2F;name&gt;\n        &lt;value&gt;1&lt;&#x2F;value&gt;\n    &lt;&#x2F;property&gt;\n&lt;&#x2F;configuration&gt;</code></pre></li>\n<li><p>修改mapred-site.xml，设置mapreduce使用的资源调度框架</p>\n<pre class=\"line-numbers language-shell\" data-language=\"shell\"><code class=\"language-shell\">[root@bigdata-01 hadoop]# vim mapred-site.xml \n&lt;configuration&gt;\n    &lt;property&gt;\n        &lt;name&gt;mapreduce.framework.name&lt;&#x2F;name&gt;\n        &lt;value&gt;yarn&lt;&#x2F;value&gt;\n    &lt;&#x2F;property&gt;\n&lt;&#x2F;configuration&gt;</code></pre></li>\n<li><p>修改yarn-site.xml，设置yarn上支持运行的服务和环境变量白名单</p>\n<pre class=\"line-numbers language-shell\" data-language=\"shell\"><code class=\"language-shell\">[root@bigdata-01 hadoop]# vim yarn-site.xml\n&lt;configuration&gt;\n    &lt;property&gt;\n        &lt;name&gt;yarn.nodemanager.aux-services&lt;&#x2F;name&gt;\n        &lt;value&gt;mapreduce_shuffle&lt;&#x2F;value&gt;\n    &lt;&#x2F;property&gt;\n    &lt;property&gt;\n        &lt;name&gt;yarn.nodemanager.env-whitelist&lt;&#x2F;name&gt;\n   &lt;value&gt;JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME&lt;&#x2F;value&gt;\n    &lt;&#x2F;property&gt;\n&lt;&#x2F;configuration&gt;</code></pre></li>\n<li><p>修改workers，设置集群中从节点的主机名信息，在这里就一台集群，所以就填写bigdata-01即可</p>\n<pre class=\"line-numbers language-shell\" data-language=\"shell\"><code class=\"language-shell\">[root@bigdata-01 hadoop]# vim workers \nbigdata-01</code></pre></li>\n</ul>\n</li>\n<li><p>格式化HDFS</p>\n<pre class=\"line-numbers language-shell\" data-language=\"shell\"><code class=\"language-shell\">[root@bigdata-01 hadoop]# hdfs namenode -format\n\n2022-05-30 15:16:10,058 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\n2022-05-30 15:16:10,060 INFO util.GSet: Computing capacity for map NameNodeRetryCache\n2022-05-30 15:16:10,060 INFO util.GSet: VM type       &#x3D; 64-bit\n2022-05-30 15:16:10,060 INFO util.GSet: 0.029999999329447746% max memory 235.9 MB &#x3D; 72.5 KB\n2022-05-30 15:16:10,060 INFO util.GSet: capacity      &#x3D; 2^13 &#x3D; 8192 entries\n2022-05-30 15:16:10,220 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1971366973-192.168.126.200-1653894970205\n2022-05-30 15:16:10,231 INFO common.Storage: Storage directory &#x2F;data&#x2F;hadoop_repo&#x2F;dfs&#x2F;name has been successfully formatted.\n2022-05-30 15:16:10,244 INFO namenode.FSImageFormatProtobuf: Saving image file &#x2F;data&#x2F;hadoop_repo&#x2F;dfs&#x2F;name&#x2F;current&#x2F;fsimage.ckpt_0000000000000000000 using no compression\n2022-05-30 15:16:10,381 INFO namenode.FSImageFormatProtobuf: Image file &#x2F;data&#x2F;hadoop_repo&#x2F;dfs&#x2F;name&#x2F;current&#x2F;fsimage.ckpt_0000000000000000000 of size 399 bytes saved in 0 seconds .\n2022-05-30 15:16:10,391 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid &gt;&#x3D; 0\n2022-05-30 15:16:10,398 INFO namenode.NameNode: SHUTDOWN_MSG: \n&#x2F;************************************************************\nSHUTDOWN_MSG: Shutting down NameNode at bigdata-01&#x2F;192.168.126.200\n************************************************************&#x2F;</code></pre>\n\n<p>如果能看到successfully formatted这条信息就说明格式化成功了。</p>\n<p>如果提示错误，一般都是因为配置文件的问题，当然需要根据具体的报错信息去分析问题。</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>注意：格式化操作只能执行一次，如果格式化的时候失败了，可以修改配置文件后再执行格式化，如果格式化成功了就不能再重复执行了，否则集群就会出现问题。</p>\n<p>如果确实需要重复执行，那么需要把/data/hadoop_repo目录中的内容全部删除，再执行格式化</p></blockquote>\n</li>\n<li><p>启动伪分布集群</p>\n<ul>\n<li><p>使用sbin目录下的start-all.sh脚本</p>\n<pre class=\"line-numbers language-shell\" data-language=\"shell\"><code class=\"language-shell\">[root@bigdata-01 hadoop]# start-all.sh \nStarting namenodes on [bigdata-01]\nERROR: Attempting to operate on hdfs namenode as root\nERROR: but there is no HDFS_NAMENODE_USER defined. Aborting operation.\nStarting datanodes\nERROR: Attempting to operate on hdfs datanode as root\nERROR: but there is no HDFS_DATANODE_USER defined. Aborting operation.\nStarting secondary namenodes [bigdata-01]\nERROR: Attempting to operate on hdfs secondarynamenode as root\nERROR: but there is no HDFS_SECONDARYNAMENODE_USER defined. Aborting operation.\nStarting resourcemanager\nERROR: Attempting to operate on yarn resourcemanager as root\nERROR: but there is no YARN_RESOURCEMANAGER_USER defined. Aborting operation.\nStarting nodemanagers\nERROR: Attempting to operate on yarn nodemanager as root\nERROR: but there is no YARN_NODEMANAGER_USER defined. Aborting operation.</code></pre>\n\n<p>解决方案如下：</p>\n<p>修改sbin目录下的start-dfs.sh，stop-dfs.sh这两个脚本文件，在文件前面增加如下内容</p>\n<pre class=\"line-numbers language-shell\" data-language=\"shell\"><code class=\"language-shell\">[root@bigdata-01 sbin]# vim start-dfs.sh \nHDFS_DATANODE_USER&#x3D;root\nHDFS_DATANODE_SECURE_USER&#x3D;hdfs\nHDFS_NAMENODE_USER&#x3D;root\nHDFS_SECONDARYNAMENODE_USER&#x3D;root\n[root@bigdata-01 sbin]# vim stop-dfs.sh \nHDFS_DATANODE_USER&#x3D;root\nHDFS_DATANODE_SECURE_USER&#x3D;hdfs\nHDFS_NAMENODE_USER&#x3D;root\nHDFS_SECONDARYNAMENODE_USER&#x3D;root</code></pre>\n\n<p>修改sbin目录下的start-yarn.sh，stop-yarn.sh这两个脚本文件，在文件前面增加如下内容</p>\n<pre class=\"line-numbers language-shell\" data-language=\"shell\"><code class=\"language-shell\">[root@bigdata-01 sbin]# vim start-yarn.sh \nYARN_RESOURCEMANAGER_USER&#x3D;root\nHADOOP_SECURE_DN_USER&#x3D;yarn\nYARN_NODEMANAGER_USER&#x3D;root\n[root@bigdata-01 sbin]# vim stop-yarn.sh\nYARN_RESOURCEMANAGER_USER&#x3D;root\nHADOOP_SECURE_DN_USER&#x3D;yarn\nYARN_NODEMANAGER_USER&#x3D;root</code></pre>\n\n<p>再次启动Hadoop</p>\n<pre class=\"line-numbers language-shell\" data-language=\"shell\"><code class=\"language-shell\">[root@bigdata-01 sbin]# start-all.sh \nStarting namenodes on [bigdata-01]\n上一次登录：一 5月 30 14:59:58 CST 2022从 192.168.126.1pts&#x2F;3 上\nStarting datanodes\n上一次登录：一 5月 30 15:30:52 CST 2022pts&#x2F;3 上\nStarting secondary namenodes [bigdata-01]\n上一次登录：一 5月 30 15:30:56 CST 2022pts&#x2F;3 上\nStarting resourcemanager\n上一次登录：一 5月 30 15:31:02 CST 2022pts&#x2F;3 上\nStarting nodemanagers\n上一次登录：一 5月 30 15:31:10 CST 2022pts&#x2F;3 上</code></pre></li>\n</ul>\n</li>\n<li><p>验证集群进程信息</p>\n<p>执行jps命令可以查看集群的进程信息，去掉Jps这个进程之外还需要有5个进程才说明集群是正常启动的</p>\n<pre class=\"line-numbers language-shell\" data-language=\"shell\"><code class=\"language-shell\">[root@bigdata-01 sbin]# jps\n2689 DataNode\n3159 ResourceManager\n3287 NodeManager\n3623 Jps\n2921 SecondaryNameNode\n2557 NameNode</code></pre>\n\n<p>还可以通过webui界面来验证集群服务是否正常</p>\n<ul>\n<li>HDFS webui界面：<a href=\"http://192.168.126.200:9870/\">http://192.168.126.200:9870</a></li>\n<li>YARN webui界面：<a href=\"http://192.168.126.200:8088/\">http://192.168.126.200:8088</a></li>\n</ul>\n</li>\n<li><p>停止集群</p>\n<p>如果修改了集群的配置文件或者是其它原因要停止集群，可以使用下面命令</p>\n<pre class=\"line-numbers language-shell\" data-language=\"shell\"><code class=\"language-shell\">[root@bigdata-01 sbin]# stop-all.sh \nStopping namenodes on [bigdata-01]\n上一次登录：一 5月 30 15:31:12 CST 2022pts&#x2F;3 上\nStopping datanodes\n上一次登录：一 5月 30 15:35:14 CST 2022pts&#x2F;3 上\nStopping secondary namenodes [bigdata-01]\n上一次登录：一 5月 30 15:35:17 CST 2022pts&#x2F;3 上\nStopping nodemanagers\n上一次登录：一 5月 30 15:35:20 CST 2022pts&#x2F;3 上\nStopping resourcemanager\n上一次登录：一 5月 30 15:35:24 CST 2022pts&#x2F;3 上</code></pre></li>\n</ol>\n","text":" Hadoop伪分布式集群即在一台Linux机器上部署Hadoop集群，通过开通不同的端口来实现相应集群搭建。 Hadoop伪分布式集群安装 这张图代表是一台Linux机器，也可以称为是一个节点，上面安装的有JDK环境 最上面的是Hadoop集群会启动的进程，其中NameNode...","link":"","photos":[],"count_time":{"symbolsCount":"7.8k","symbolsTime":"7 mins."},"categories":[{"name":"Hadoop","slug":"Hadoop","count":2,"path":"api/categories/Hadoop.json"}],"tags":[{"name":"Hadoop","slug":"Hadoop","count":2,"path":"api/tags/Hadoop.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#Hadoop%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85\"><span class=\"toc-text\">Hadoop伪分布式集群安装</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E9%85%8D%E7%BD%AE%E5%9F%BA%E7%A1%80%E7%8E%AF%E5%A2%83\"><span class=\"toc-text\">配置基础环境</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E5%AE%89%E8%A3%85Hadoop\"><span class=\"toc-text\">安装Hadoop</span></a></li></ol></li></ol>","author":{"name":"张春博","slug":"blog-author","avatar":"/images/QD20000163.jpg","link":"/","description":"研发及交付中心<br>零售开发组<br>手机银行","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"mapped":true,"prev_post":{"title":"2023年2月03工作周报","uid":"f65eea67ff289535800c5f32ff421958","slug":"2023年2月03工作周报","date":"2023-02-03T09:31:10.000Z","updated":"2023-03-06T04:31:54.215Z","comments":true,"path":"api/articles/2023年2月03工作周报.json","keywords":null,"cover":"/post/2023年2月03工作周报/cover.jpeg","text":"手机银行自研工作： 在建需求7单：1单完成上线，2单系统测试，2单开发编码中，2单需求分析中。 【需求-紧急】ALM00025739-关于手机银行隐私协议更新的需求：完成上线 手机银行专题工作 1月份专题：完成三次灰度的发布。 2月份专题：确定发布范围。 完成22年/23年手机银...","link":"","photos":[],"count_time":{"symbolsCount":178,"symbolsTime":"1 mins."},"categories":[{"name":"工作","slug":"工作","count":8,"path":"api/categories/工作.json"}],"tags":[{"name":"工作日志","slug":"工作日志","count":8,"path":"api/tags/工作日志.json"}],"author":{"name":"张春博","slug":"blog-author","avatar":"/images/QD20000163.jpg","link":"/","description":"研发及交付中心<br>零售开发组<br>手机银行","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}}},"next_post":{"title":"ES 之分词与内置分词器","uid":"18af339b30adc2c46fa137e5f29a4b24","slug":"ES 之分词与内置分词器","date":"2022-02-16T12:23:29.000Z","updated":"2022-02-16T12:35:24.127Z","comments":true,"path":"api/articles/ES 之分词与内置分词器.json","keywords":null,"cover":"/post/ES 之分词与内置分词器/cover.jpeg","text":"分词器是从一串文本中切分一个个的词条，并对每个词条进行标准化。 什么是分词？把文本转换为一个个的单词，分词称之为analysis。es默认只对英文语句做分词，中文不支持，每个中文字都会被拆分为独立的个体。 英文分词：I study in imooc.com 中文分词：我在慕课网学...","link":"","photos":[],"count_time":{"symbolsCount":763,"symbolsTime":"1 mins."},"categories":[{"name":"ELK","slug":"ELK","count":3,"path":"api/categories/ELK.json"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","count":3,"path":"api/tags/Elasticsearch.json"}],"author":{"name":"张春博","slug":"blog-author","avatar":"/images/QD20000163.jpg","link":"/","description":"研发及交付中心<br>零售开发组<br>手机银行","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}}}}