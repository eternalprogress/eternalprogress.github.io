{"title":"数据仓库Hive入门","uid":"c0a885913ce34c6575f9b0ea8cad6584","slug":"数据仓库Hive入门","date":"2022-06-01T08:23:04.000Z","updated":"2023-03-01T07:01:52.439Z","comments":true,"path":"api/articles/数据仓库Hive入门.json","keywords":null,"cover":"/post/数据仓库Hive入门/cover.jpeg","content":"<hr>\n<p>Hive是建立在Hadoop上的数据仓库基础构架。它提供了一系列的工具，可以用来进行数据提取转化加载，可以简称为ETL。</p>\n<hr>\n<h3 id=\"什么是Hive\"><a href=\"#什么是Hive\" class=\"headerlink\" title=\"什么是Hive\"></a>什么是Hive</h3><ul>\n<li>Hive 定义了简单的类SQL查询语言，称为HQL，它允许熟悉SQL的用户直接查询Hadoop中的数据，同时，这个语言也允许熟悉MapReduce的开发者开发自定义的mapreduce任务来处理内建的SQL函数无法完成的复杂的分析任务</li>\n<li>Hive中包含的有SQL解析引擎，它会将SQL语句转译成M/R Job,然后在Hadoop中执行。</li>\n<li>通过这里的分析我们可以了解到Hive可以通过sql查询Hadoop中的数据，并且sql底层也会转化成mapreduce任务，所以hive是基于hadoop的。</li>\n</ul>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>简单来说Hive，是MR的客户端，也就是说不必要每台机器都安装部署Hive</p></blockquote>\n<h3 id=\"Hive的数据存储\"><a href=\"#Hive的数据存储\" class=\"headerlink\" title=\"Hive的数据存储\"></a>Hive的数据存储</h3><ul>\n<li>Hive的数据存储基于Hadoop的 HDFS</li>\n<li>Hive没有专门的数据存储格式</li>\n<li>Hive默认可以直接加载文本文件（TextFile），还支持SequenceFile、RCFile等文件格式</li>\n<li>针对普通文本数据，我们在创建表时，只需要指定数据的列分隔符与行分隔符，Hive即可解析里面的数据</li>\n</ul>\n<h3 id=\"Hive的系统架构\"><a href=\"#Hive的系统架构\" class=\"headerlink\" title=\"Hive的系统架构\"></a>Hive的系统架构</h3><p><img src=\"/post/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93Hive%E5%85%A5%E9%97%A8/x_16.png\" alt=\"在这里插入图片描述\"></p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><ul>\n<li>用户接口，包括 CLI、JDBC/ODBC、WebGUI、CLI，即Shell命令行，表示我们可以通过shell命令行操作Hive JDBC/ODBC 是 Hive 的Java操作方式，与使用传统数据库JDBC的方式类似</li>\n<li>元数据存储(Metastore)，注意：这里的存储是名词，Metastore表示是一个存储系统 Hive中的元数据包括表的相关信息，Hive会将这些元数据存储在Metastore中，目前Metastore只支持 mysql、derby。</li>\n<li>Driver：包含：编译器、优化器、执行器、编译器、优化器、执行器可以完成 Hive的 查询语句从词法分析、语法分析、编译、优化以及查询计划的生成。生成的查询计划最终存储在 HDFS 中，并在随后由 MapReduce 调用执行</li>\n<li>Hadoop：Hive会使用 HDFS 进行存储，利用 MapReduce 进行计算Hive 的数据存储在 HDFS 中，大部分的查询由 MapReduce 完成（特例 select * from table 不会生成 MapRedcue 任务，如果在SQL语句后面再增加where过滤条件就会生成MapReduce任务）</li>\n</ul></blockquote>\n<p>在这有一点需要注意的，就是从Hive2开始，其实官方就不建议默认使用MapReduce引擎了，而是建议使用Tez引擎或者是Spark引擎，不过目前一直到最新的3.x版本中mapreduce还是默认的执行引擎</p>\n<p>其实大数据计算引擎是有几个发展阶段的</p>\n<h3 id=\"大数据计算引擎\"><a href=\"#大数据计算引擎\" class=\"headerlink\" title=\"大数据计算引擎\"></a>大数据计算引擎</h3><h4 id=\"1、MapReduce\"><a href=\"#1、MapReduce\" class=\"headerlink\" title=\"1、MapReduce\"></a>1、MapReduce</h4><p>首先是第一代大数据计算引擎：MapReduce</p>\n<h4 id=\"2、Tez\"><a href=\"#2、Tez\" class=\"headerlink\" title=\"2、Tez\"></a>2、Tez</h4><p>接着是第二代大数据计算引擎：Tez<br>Tez的存在感比较低，它是源于MapReduce，主要和Hive结合在一起使用，它的核心思想是将Map和Reduce两个操作进一步拆分，这些分解后的元操作可以灵活组合，产生新的操作，这些操作经过一些控制程序组装后，可以形成一个大的作业，这样可以提高计算效率，我们在实际工作中Hive使用的就是 Tez引擎，替换Hive的执行引擎也很简单，只需要把Tez安装好（Tez也是支持在YARN上执行的），然后到Hive中配置一下就可以了，不管使用什么引擎，不会对我们使用hive造成什么影响，也就说对上层的使用没有影响</p>\n<h4 id=\"3、Spark\"><a href=\"#3、Spark\" class=\"headerlink\" title=\"3、Spark\"></a>3、Spark</h4><p>接着是第三代大数据计算引擎：Spark<br>Spark在当时属于一个划时代的产品，改变了之前基于磁盘的计算思路，而是采用内存计算，就是说Spark把数据读取过来以后，中间的计算结果是不会进磁盘的，一直到出来最终结果，才会写磁盘，这样就大大提高了计算效率，而MapReduce的中间结果是会写磁盘的，所以效率没有Spark高。Spark的执行效率号称比MapReduce 快100倍，当然这需要在一定数据规模下才会差这么多，如果我们就计算几十兆或者几百兆的文件，你去对比发现其实也不会差多少，后面我们也会学到Spark这个基于内存的大数据计算引擎<br>注意：spark也是支持在YARN上执行的</p>\n<h4 id=\"4、Flink\"><a href=\"#4、Flink\" class=\"headerlink\" title=\"4、Flink\"></a>4、Flink</h4><p>还有第四代大数据计算引擎：Flink<br>Flink是一个可以支持纯实时数据计算的计算引擎，在实时计算领域要优于Saprk，Flink和Spark其实是有很多相似之处，在某些方面他们两个属于互相参考，互相借鉴，互相成长，Flink后面我们也会学到，等后面我们讲到这个计算引擎的时候再详细分析。<br>注意：Flink也是支持在YARN上执行的。</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>所以发现没有，MapReduce、Tez、Spark、Flink这些计算引擎都是支持在yarn上执行的，所以说 Hdoop2中对架构的拆分是非常明智的。</p>\n<p>解释完这些名词之后其实我们就对这个架构有了一个基本理解，<br> 再看来这个图<br> 用户通过接口传递Hive SQL，然后经过Driver对SQL进行分析、编译，生成查询计划，查询计划会存储在 HDFS中，然后再通过MapReduce进行计算出结果，这就是整个大的流程。<br> 其实在这里我们可以发现，Hive这个哥们是既不存储数据，也不计算数据，这些活都给了Hadoop来干， Hive底层最核心的东西其实就是Driver这一块，将SQL语句解析为最终的查询计划。</p></blockquote>\n<h3 id=\"Metastore\"><a href=\"#Metastore\" class=\"headerlink\" title=\"Metastore\"></a>Metastore</h3><p>接着来看一下Hive中的元数据存储，<strong>Metastore</strong><br> Metastore是<strong>Hive元数据的集中存放地</strong>。<br> Hive 中的元数据包括表的名字，表的列和分区及其属性，表的属性（是否为外部表等），表的数据所在 的hdfs目录等<br> Metastore默认使用内嵌的derby数据库<br> Derby数据库的缺点：在同一个目录下一次只能打开一个会话<br> 使用derby 存储方式时， Hive 会在当前目录生成一个derby.log 文件和一个metastore_db 目录， metastore_db里面会存储具体的元数据信息<br> 没有办法使用之前的元数据信息了。<br> 推荐使用<strong>MySQL作为外置存储引擎</strong>，可以支持<strong>多用户同时访问以及元数据共享</strong>。</p>\n<h3 id=\"数据仓库和数据库的区别\"><a href=\"#数据仓库和数据库的区别\" class=\"headerlink\" title=\"数据仓库和数据库的区别\"></a>数据仓库和数据库的区别</h3><h4 id=\"Hive-VS-Mysql\"><a href=\"#Hive-VS-Mysql\" class=\"headerlink\" title=\"Hive VS Mysql\"></a>Hive VS Mysql</h4><p>为了加深对Hive的理解，下面我们拿Hive和我们经常使用的Mysql做一个对比</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th align=\"center\">Hive</th>\n<th align=\"center\">Mysql</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>数据存储位置</td>\n<td align=\"center\">HDFS</td>\n<td align=\"center\">本地磁盘</td>\n</tr>\n<tr>\n<td>数据格式</td>\n<td align=\"center\">用户定义</td>\n<td align=\"center\">系统决定</td>\n</tr>\n<tr>\n<td>数据更新</td>\n<td align=\"center\">不支持（不支持修改和删除）</td>\n<td align=\"center\">支持（增删改查）</td>\n</tr>\n<tr>\n<td>索引</td>\n<td align=\"center\">有，但较弱，一般很少用</td>\n<td align=\"center\">有，经常使用</td>\n</tr>\n<tr>\n<td>执行</td>\n<td align=\"center\">MapReduce</td>\n<td align=\"center\">Executor</td>\n</tr>\n<tr>\n<td>执行延迟</td>\n<td align=\"center\">高</td>\n<td align=\"center\">低</td>\n</tr>\n<tr>\n<td>可扩展性</td>\n<td align=\"center\">高</td>\n<td align=\"center\">低</td>\n</tr>\n<tr>\n<td>数据规模</td>\n<td align=\"center\">大</td>\n<td align=\"center\">小</td>\n</tr>\n</tbody></table>\n<h4 id=\"数据仓库-VS-数据库\"><a href=\"#数据仓库-VS-数据库\" class=\"headerlink\" title=\"数据仓库 VS 数据库\"></a>数据仓库 VS 数据库</h4><p>前面我们说了Hive是一个数据仓库，咱们平时经常使用的mysql属于数据库，那数据库和数据仓库到底有什么区别呢？<br>下面我们来分析一下</p>\n<ul>\n<li><strong>数据库</strong>：传统的关系型数据库主要应用在基本的事务处理，例如银行交易之类的场景<br>数据库支持增删改查这些常见的操作。（mysql、oracle、sqlserver、DB2、sqlite、MDB）</li>\n<li><strong>数据仓库</strong>：主要做一些复杂的分析操作，侧重决策支持，相对数据库而言，数据仓库分析的数据规模要大得多。但是数据仓库只支持查询操作，不支持修改和删除。（Hive，相当于MR的客户端）</li>\n</ul>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p><strong>其实数据库与数据仓库的本质区别就是 OLTP与OLAP 的区别</strong></p></blockquote>\n<h4 id=\"OLTP-VS-OLAP\"><a href=\"#OLTP-VS-OLAP\" class=\"headerlink\" title=\"OLTP VS OLAP\"></a>OLTP VS OLAP</h4><p>那这里的OLTO和OLAP又是什么意思呢？<br> **OLTP(On-Line Transaction Processing)**：操作型处理，称为联机事务处理，也可以称为面向交易的处理系统，它是针对具体业务在数据库联机的日常操作，通常对少数记录进行查询、修改。用户较为关心操作的响应时间、数据的安全性、完整性等问题<br>**OLAP(On-Line Analytical Processing)**：分析型处理，称为联机分析处理，一般针对某些主题历史数据进行分析，支持管理决策。</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>其实从字面上来对比，OLTP 和 OLAP 只有一个单词不一样<br>OLTP侧重于事务，OLAP侧重于分析<br>所以数据库和数据仓库的特性是不一样的，不过我们平时在使用的时候，可以把Hive作为一个数据库来操作，但是你要知道他们两个是不一样的。数据仓库的概念是比数据库要大的</p></blockquote>\n<h3 id=\"Hive的安装和部署\"><a href=\"#Hive的安装和部署\" class=\"headerlink\" title=\"Hive的安装和部署\"></a>Hive的安装和部署</h3><h4 id=\"1、下载Hive安装包\"><a href=\"#1、下载Hive安装包\" class=\"headerlink\" title=\"1、下载Hive安装包\"></a>1、下载Hive安装包</h4><p>首先要下载Hive的安装包，进入Hive的官网，找到download下载链接。</p>\n<p><img src=\"/post/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93Hive%E5%85%A5%E9%97%A8/x_16-20220601171638688.png\" alt=\"在这里插入图片描述\"></p>\n<p>发现目前hive主要有三大版本，Hive1.x、Hive2.x、Hive3.x<br>Hive1.x已经2年没有更新了，所以这个版本后续基本不会再维护了，不过这个版本已经迭代了很多年了，也是比较稳定的<br>Hive2.x最近一直在更新<br>Hive3.x上次是19年8月份更新的，也算是一直在维护</p>\n<p>那我们到底选择哪个版本呢？注意了，在选择Hive版本的时候我们需要注意已有的Hadoop集群的版本。因为Hive会依赖于Hadoop，所以版本需要兼容才可以。</p>\n<p>具体Hive和Hadoop的版本对应关系可以在download页面下的news列表里面看到。</p>\n<p><a href=\"https://hive.apache.org/downloads.html\">https://hive.apache.org/downloads.html</a></p>\n<p><img src=\"/post/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93Hive%E5%85%A5%E9%97%A8/x_16-20220601171903328.png\" alt=\"在这里插入图片描述\"></p>\n<p>按照这里面说的hive2.x的需要在hadoop2.x版本中运行，hive3.x的需要在hadoop3.x版本中运行。<br>所以在这里我们最好是使用Hive3.x的版本</p>\n<p>那我们就下载hive-3.1.2这个版本，如果想要下载其它历史版本的话这里面还找不到，不过可以使用apache的一个通用archive地址<br><a href=\"https://archive.apache.org/dist/hive/\">https://archive.apache.org/dist/hive/</a></p>\n<p>在这里面就可以找到hive的所有历史版本了</p>\n<h4 id=\"2、克隆bd04安装hive并配置为hadoop集群客户端\"><a href=\"#2、克隆bd04安装hive并配置为hadoop集群客户端\" class=\"headerlink\" title=\"2、克隆bd04安装hive并配置为hadoop集群客户端\"></a>2、克隆bd04安装hive并配置为hadoop集群客户端</h4><pre class=\"line-numbers language-shell\" data-language=\"shell\"><code class=\"language-shell\">bd04 192.168.126.204</code></pre>\n\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>编写xsyncforhost脚本，作用向指定的host机器同步指定的文件或者文件夹</p>\n<pre class=\"line-numbers language-shell\" data-language=\"shell\"><code class=\"language-shell\">#!&#x2F;bin&#x2F;bash\n# $#：表示传递给脚本或函数的参数个数。\n#1 获取输入参数个数，如果没有参数，直接退出\npcount&#x3D;$#\nif((pcount&#x3D;&#x3D;0)); then\necho no args;\nexit;\nfi\n\n#2 获取文件名称\np1&#x3D;$1\nfname&#x3D;&#96;basename $p1&#96;\necho fname&#x3D;$fname\n\n#3 获取上级目录到绝对路径\npdir&#x3D;&#96;cd -P $(dirname $p1); pwd&#96;\necho pdir&#x3D;$pdir\n\n#4 获取当前用户名称\nuser&#x3D;&#96;whoami&#96;\n\n#5 循环\n       echo --------------- $2 ----------------\n       rsync -rvl $pdir&#x2F;$fname $user@$2:$pdir</code></pre>\n\n<p>同步bd01hadoop到bd04</p>\n<pre class=\"line-numbers language-shell\" data-language=\"shell\"><code class=\"language-shell\">[root@bd01 moudle]# xsyncforhost hadoop-3.2.0&#x2F; bd04</code></pre></blockquote>\n<h4 id=\"3、将Hive上传到bd04虚拟机，并解压\"><a href=\"#3、将Hive上传到bd04虚拟机，并解压\" class=\"headerlink\" title=\"3、将Hive上传到bd04虚拟机，并解压\"></a>3、将Hive上传到bd04虚拟机，并解压</h4><pre class=\"line-numbers language-shell\" data-language=\"shell\"><code class=\"language-shell\">[root@bd04 software]# tar -zxvf apache-hive-3.1.2-bin.tar.gz -C &#x2F;opt&#x2F;moudle&#x2F;</code></pre>\n\n<h4 id=\"4、拷贝并重命名配置文件\"><a href=\"#4、拷贝并重命名配置文件\" class=\"headerlink\" title=\"4、拷贝并重命名配置文件\"></a>4、拷贝并重命名配置文件</h4><pre class=\"line-numbers language-shell\" data-language=\"shell\"><code class=\"language-shell\">[root@bd04 conf]# cp hive-default.xml.template hive-site.xml\n[root@bd04 conf]# cp hive-env.sh.template hive-env.sh\n[root@bd04 conf]# ll\n总用量 632\n-rw-r--r--. 1 root root   1596 8月  23 2019 beeline-log4j2.properties.template\n-rw-r--r--. 1 root root 300482 8月  23 2019 hive-default.xml.template\n-rw-r--r--. 1 root root   2365 6月   2 14:38 hive-env.sh\n-rw-r--r--. 1 root root   2365 8月  23 2019 hive-env.sh.template\n-rw-r--r--. 1 root root   2274 8月  23 2019 hive-exec-log4j2.properties.template\n-rw-r--r--. 1 root root   3086 8月  23 2019 hive-log4j2.properties.template\n-rw-r--r--. 1 root root 300482 6月   2 14:38 hive-site.xml\n-rw-r--r--. 1 root root   2060 8月  23 2019 ivysettings.xml\n-rw-r--r--. 1 root root   3558 8月  23 2019 llap-cli-log4j2.properties.template\n-rw-r--r--. 1 root root   7163 8月  23 2019 llap-daemon-log4j2.properties.template\n-rw-r--r--. 1 root root   2662 8月  23 2019 parquet-logging.properties</code></pre>\n\n<h4 id=\"5、Centos安装MySQL8-0-16\"><a href=\"#5、Centos安装MySQL8-0-16\" class=\"headerlink\" title=\"5、Centos安装MySQL8.0.16\"></a>5、Centos安装MySQL8.0.16</h4><ul>\n<li><p>下载MySQL所需安装包</p>\n<p>从 <a href=\"https://link.juejin.cn/?target=https://downloads.mysql.com/archives/community/\">MySQL官网</a> 下载，上传至 CentOS 系统 <code>/usr/local/MySQL</code> 目录下，当然你也可以使用 <code>wget</code> 命令直接下载至 CentOS，此处使用的 8.0.16 版本。</p>\n<pre class=\"line-numbers language-bash\" data-language=\"bash\"><code class=\"language-bash\"># 你想要的版本\nProduct Version: 8.0.16\n# CentOS选择Red Hat Enterprise Linux &#x2F; Oracle Linux\nOperating System:OS Version: Red Hat Enterprise Linux &#x2F; Oracle Linux\n# CentOS7 64位选择\nOS Version: Red Hat Enterprise Linux 7 &#x2F; Oracle Linux 7 (x86, 64-bit)</code></pre></li>\n<li><p>检查是否存在自带mariadb</p>\n<p>CentOS7 开始不自带 MySQL，替换成了 mariadb，但是我们安装 MySQL 的时候会冲突，所以需要先卸载 mariadb。</p>\n<pre class=\"line-numbers language-shell\" data-language=\"shell\"><code class=\"language-shell\"># 查找是否存在自带mariadb\nrpm -qa | grep mariadb\n\n# 如果存在则卸载, 比如我查找出来的名称为mariadb-libs-5.5.68-1.el7.x86_64\nrpm -e mariadb-libs-5.5.68-1.el7.x86_64 --nodeps</code></pre></li>\n<li><p>检查是否安装过MySQL</p>\n<pre class=\"line-numbers language-shell\" data-language=\"shell\"><code class=\"language-shell\">[root@bd04 conf]# rpm -qa | grep mysql\n# 如果存在则卸载, 比如名称为mysql-libs-5.1.52.x86_64\nrpm -e mysql-libs-5.1.52.x86_64 --nodeps</code></pre></li>\n<li><p> 检查mysql组及用户是否存在，不存在则创建</p>\n</li>\n</ul>\n  <pre class=\"line-numbers language-shell\" data-language=\"shell\"><code class=\"language-shell\">检查\n[root@bd04 conf]# cat &#x2F;etc&#x2F;group | grep mysql\n[root@bd04 conf]# cat &#x2F;etc&#x2F;passwd | grep mysql\n# 创建\ngroupadd mysql\nuseradd -r -g mysql mysql</code></pre>\n\n<ul>\n<li><p>安装及配置MySQL</p>\n<ol>\n<li><p>解压</p>\n<pre class=\"line-numbers language-shell\" data-language=\"shell\"><code class=\"language-shell\">[root@bd04 software]# tar -xvf mysql-8.0.16-2.el7.x86_64.rpm-bundle.tar -C &#x2F;opt&#x2F;moudle&#x2F;mysql&#x2F;</code></pre></li>\n<li><p>通过rpm命令安装common</p>\n<pre class=\"line-numbers language-shell\" data-language=\"shell\"><code class=\"language-shell\">[root@bd04 mysql]# rpm -ivh mysql-community-common-8.0.16-2.el7.x86_64.rpm \n警告：mysql-community-common-8.0.16-2.el7.x86_64.rpm: 头V3 DSA&#x2F;SHA1 Signature, 密钥 ID 5072e1f5: NOKEY\n准备中...                          ################################# [100%]\n正在升级&#x2F;安装...\n   1:mysql-community-common-8.0.16-2.e################################# [100%]</code></pre></li>\n<li><p>通过rpm命令安装libs</p>\n<pre class=\"line-numbers language-shell\" data-language=\"shell\"><code class=\"language-shell\">[root@bd04 mysql]# rpm -ivh mysql-community-libs-8.0.16-2.el7.x86_64.rpm \n警告：mysql-community-libs-8.0.16-2.el7.x86_64.rpm: 头V3 DSA&#x2F;SHA1 Signature, 密钥 ID 5072e1f5: NOKEY\n准备中...                          ################################# [100%]\n正在升级&#x2F;安装...\n   1:mysql-community-libs-8.0.16-2.el7################################# [100%]</code></pre></li>\n<li><p>通过rpm命令安装client</p>\n<pre class=\"line-numbers language-shell\" data-language=\"shell\"><code class=\"language-shell\">[root@bd04 mysql]# rpm -ivh mysql-community-client-8.0.16-2.el7.x86_64.rpm \n警告：mysql-community-client-8.0.16-2.el7.x86_64.rpm: 头V3 DSA&#x2F;SHA1 Signature, 密钥 ID 5072e1f5: NOKEY\n准备中...                          ################################# [100%]\n正在升级&#x2F;安装...\n   1:mysql-community-client-8.0.16-2.e################################# [100%]</code></pre></li>\n<li><p>通过rpm命令安装server</p>\n<pre class=\"line-numbers language-shell\" data-language=\"shell\"><code class=\"language-shell\">[root@bd04 mysql]# rpm -ivh mysql-community-server-8.0.16-2.el7.x86_64.rpm \n警告：mysql-community-server-8.0.16-2.el7.x86_64.rpm: 头V3 DSA&#x2F;SHA1 Signature, 密钥 ID 5072e1f5: NOKEY\n准备中...                          ################################# [100%]\n正在升级&#x2F;安装...\n   1:mysql-community-server-8.0.16-2.e################################# [100%]</code></pre></li>\n<li><p>查看MySQL安装包</p>\n<pre class=\"line-numbers language-shell\" data-language=\"shell\"><code class=\"language-shell\">[root@bd04 mysql]# rpm -qa | grep mysql\nmysql-community-server-8.0.16-2.el7.x86_64\nmysql-community-common-8.0.16-2.el7.x86_64\nmysql-community-client-8.0.16-2.el7.x86_64\nmysql-community-libs-8.0.16-2.el7.x86_64</code></pre></li>\n<li><p>初始化MySQL数据库</p>\n<p>该命令会在 <code>/var/log/mysqld.log</code> 生成随机密码。</p>\n<pre class=\"line-numbers language-shell\" data-language=\"shell\"><code class=\"language-shell\">[root@bd04 mysql]# mysqld --initialize</code></pre></li>\n<li><p>配置用户组</p>\n<p>配置MySQL数据库目录所属的用户和组，默认MySQL的配置文件路径为： <code>/etc/my.cnf</code>，如果有需要可以修改配置文件。</p>\n<pre class=\"line-numbers language-bash\" data-language=\"bash\"><code class=\"language-bash\">chown mysql:mysql &#x2F;var&#x2F;lib&#x2F;mysql -R</code></pre></li>\n<li><p>启动MySQL数据库</p>\n<pre class=\"line-numbers language-bash\" data-language=\"bash\"><code class=\"language-bash\"># 启动mysqld.service\nsystemctl start mysqld\n\n# 查看状态, 显示有 running 代表启动成功\nsystemctl status mysqld</code></pre></li>\n<li><p>修改数据库密码</p>\n<pre class=\"line-numbers language-bash\" data-language=\"bash\"><code class=\"language-bash\"># 查看数据库密码\ncat &#x2F;var&#x2F;log&#x2F;mysqld.log | grep password\n\n# 打印内容, 其中w:rwf8Xvf%kw就是数据库密码\n2022-06-02T07:21:55.946368Z 5 [Note] [MY-010454] [Server] A temporary password is generated for root@localhost: w:rwf8Xvf%kw</code></pre>\n\n<pre class=\"line-numbers language-shell\" data-language=\"shell\"><code class=\"language-shell\"># 使用查询到的密码登录MySQL\nmysql -u root -p&#39;w:rwf8Xvf%kw&#39;\n\n# 登录到MySQL之后修改密码为自定义密码(我设置为123456), 这个设置还和5.7版本不同呢\nALTER USER &#39;root&#39;@&#39;localhost&#39; IDENTIFIED WITH mysql_native_password BY &#39;admin&#39;;\n\n# 使用&#96;exit;&#96;退出MySQL后使用自定义密码重新登录</code></pre></li>\n<li><p>创建用户并授权远程登录</p>\n<p>此处不能使用 5.7 版本的方式，不然会报错。</p>\n<pre class=\"line-numbers language-none\"><code class=\"language-none\">mysql&gt; GRANT ALL PRIVILEGES ON *.* TO &#39;SunnyBear&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;;\nERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near &#39;IDENTIFIED BY &#39;123456&#39;&#39; at line 1\n复制代码</code></pre>\n\n<p><strong>8.0版本需要使用如下方式</strong>，和 Oralce 的创建授权类似，先创建用户再赋予权限。</p>\n<p>因为正常开发一般 root 用户不允许远程登录，所以我们创建一个 SunnyBear 用户，赋予使用 sunny 数据库(已经使用命令创建好)的权限。</p>\n<pre class=\"line-numbers language-sql\" data-language=\"sql\"><code class=\"language-sql\"># 先创建一个用户\nmysql&gt; CREATE USER &#39;SunnyBear&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;;\nQuery OK, 0 rows affected (0.00 sec)\n\n# 授权\n# 其中sunny.*代表赋予sunny数据库所有操作权限, 如果想赋予所有数据库权限, 可设置为*.*\n# 其中&#39;SunnyBear&#39;@&#39;%&#39;代表允许SunnyBear用户在任何ip登录, 当然也可以指定ip, 例如&#39;用户名称&#39;@&#39;ip地址&#39;\nmysql&gt; GRANT ALL PRIVILEGES ON sunny.* TO &#39;SunnyBear&#39;@&#39;%&#39; WITH GRANT OPTION;\nQuery OK, 0 rows affected (0.00 sec)\n\n# 刷新权限\nFLUSH PRIVILEGES;</code></pre>\n\n<p>配置成功之后可以使用连接工具尝试连接，我这里使用的是 Navicat，至此，单机版 MySQL 安装配置完成。</p>\n</li>\n<li><p>一些命令</p>\n<pre class=\"line-numbers language-shell\" data-language=\"shell\"><code class=\"language-shell\"># 启动mysql服务\nsystemctl start mysqld.service\n\n# 停止mysql服务\nsystemctl stop mysqld.service\n\n# 重启mysql服务\nsystemctl restart mysqld.service\n\n# 查看mysql服务当前状态\nsystemctl status mysqld.service\n\n# 设置mysql服务开机自启动\nsystemctl enable mysqld.service\n\n# 停止mysql服务开机自启动\nsystemctl disable mysqld.service</code></pre></li>\n</ol>\n</li>\n</ul>\n<h4 id=\"6、修改配置文件\"><a href=\"#6、修改配置文件\" class=\"headerlink\" title=\"6、修改配置文件\"></a>6、修改配置文件</h4><ul>\n<li><p>修改hive-env.sh文件</p>\n<pre class=\"line-numbers language-shell\" data-language=\"shell\"><code class=\"language-shell\">[root@bd04 conf]# vim hive-env.sh\n\nexport JAVA_HOME&#x3D;&#x2F;opt&#x2F;moudle&#x2F;jdk1.8.0_191\nexport HIVE_HOME&#x3D;&#x2F;opt&#x2F;moudle&#x2F;apache-hive-3.1.2-bin\nexport HADOOP_HOME&#x3D;&#x2F;opt&#x2F;moudle&#x2F;hadoop-3.2.0</code></pre></li>\n<li><p>修改hive-site.xml</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>在hive-site.xml文件中根据下面property中的name属性的值修改对应value的值，这些属性默认里面都是有的，所以都是修改对应的value的值即可</p></blockquote>\n<pre class=\"line-numbers language-shell\" data-language=\"shell\"><code class=\"language-shell\">[root@bd04 conf]# vim hive-site.xml \n&lt;property&gt;\n    &lt;name&gt;javax.jdo.option.ConnectionURL&lt;&#x2F;name&gt;\n    &lt;value&gt;jdbc:mysql:&#x2F;&#x2F;bd04:3306&#x2F;hive?serverTimezone&#x3D;Asia&#x2F;Shanghai&lt;&#x2F;value&gt;\n  &lt;&#x2F;property&gt;\n  &lt;property&gt; \n    &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;&#x2F;name&gt; \n    &lt;value&gt;com.mysql.cj.jdbc.Driver&lt;&#x2F;value&gt; \n&lt;&#x2F;property&gt; \n&lt;property&gt; \n    &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;&#x2F;name&gt; \n    &lt;value&gt;root&lt;&#x2F;value&gt;\n&lt;&#x2F;property&gt; \n&lt;property&gt; \n    &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;&#x2F;name&gt;\n    &lt;value&gt;admin&lt;&#x2F;value&gt;\n&lt;&#x2F;property&gt;\n&lt;property&gt; \n    &lt;name&gt;hive.querylog.location&lt;&#x2F;name&gt; \n    &lt;value&gt;&#x2F;data&#x2F;hive_repo&#x2F;querylog&lt;&#x2F;value&gt;\n&lt;&#x2F;property&gt; \n&lt;property&gt; \n    &lt;name&gt;hive.exec.local.scratchdir&lt;&#x2F;name&gt; \n    &lt;value&gt;&#x2F;data&#x2F;hive_repo&#x2F;scratchdir&lt;&#x2F;value&gt;\n&lt;&#x2F;property&gt; \n&lt;property&gt; \n    &lt;name&gt;hive.downloaded.resources.dir&lt;&#x2F;name&gt;\n    &lt;value&gt;&#x2F;data&#x2F;hive_repo&#x2F;resources&lt;&#x2F;value&gt;\n&lt;&#x2F;property&gt;</code></pre>\n\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>注意:mysql驱动包的版本，要和我们安装的版本保持一致：mysql-connector-java-8.0.16.jar,这里需要注意mysql驱动需要放在hive下面的lib包下。</p></blockquote>\n<pre class=\"line-numbers language-shell\" data-language=\"shell\"><code class=\"language-shell\">[root@bd04 lib]# pwd\n&#x2F;opt&#x2F;moudle&#x2F;apache-hive-3.1.2-bin&#x2F;lib\n[root@bd04 lib]# ll | grep mysql\n-rw-r--r--. 1 root root  2293144 6月   2 15:39 mysql-connector-java-8.0.16.jar\n-rw-r--r--. 1 root root    10476 11月 16 2018 mysql-metadata-storage-0.12.0.jar</code></pre></li>\n</ul>\n<h4 id=\"7、修改core-site-xml的配置\"><a href=\"#7、修改core-site-xml的配置\" class=\"headerlink\" title=\"7、修改core-site.xml的配置\"></a>7、修改core-site.xml的配置</h4><p>修改bigdata01中的core-site.xml，然后同步到集群中的另外两个节点上<br>如果不增加这个配置，使用beeline连接hive的时候会报错</p>\n<pre class=\"line-numbers language-shell\" data-language=\"shell\"><code class=\"language-shell\">[root@bd01 hadoop]# vim core-site.xml \n  &lt;property&gt;\n        &lt;name&gt;hadoop.proxyuser.root.hosts&lt;&#x2F;name&gt;\n        &lt;value&gt;*&lt;&#x2F;value&gt;\n    &lt;&#x2F;property&gt;\n    &lt;property&gt;\n        &lt;name&gt;hadoop.proxyuser.root.groups&lt;&#x2F;name&gt;\n        &lt;value&gt;*&lt;&#x2F;value&gt;\n&lt;&#x2F;property&gt;\n[root@bd01 hadoop]# xsync core-site.xml \nfname&#x3D;core-site.xml\npdir&#x3D;&#x2F;opt&#x2F;moudle&#x2F;hadoop-3.2.0&#x2F;etc&#x2F;hadoop\n--------------- bd01 ----------------\nsending incremental file list\n\nsent 50 bytes  received 12 bytes  124.00 bytes&#x2F;sec\ntotal size is 1,191  speedup is 19.21\n--------------- bd02 ----------------\nsending incremental file list\ncore-site.xml\n\nsent 592 bytes  received 47 bytes  426.00 bytes&#x2F;sec\ntotal size is 1,191  speedup is 1.86\n--------------- bd03 ----------------\nsending incremental file list\ncore-site.xml\n\nsent 592 bytes  received 47 bytes  426.00 bytes&#x2F;sec\ntotal size is 1,191  speedup is 1.86</code></pre>\n\n<h4 id=\"8、启动Hadoop集群\"><a href=\"#8、启动Hadoop集群\" class=\"headerlink\" title=\"8、启动Hadoop集群\"></a>8、启动Hadoop集群</h4><pre class=\"line-numbers language-shell\" data-language=\"shell\"><code class=\"language-shell\">[root@bd01 hadoop]# start-all.sh \nStarting namenodes on [bd01]\n上一次登录：四 6月  2 15:42:23 CST 2022从 192.168.126.1pts&#x2F;0 上\nStarting datanodes\n上一次登录：四 6月  2 15:46:26 CST 2022pts&#x2F;0 上\nStarting secondary namenodes [bd01]\n上一次登录：四 6月  2 15:46:29 CST 2022pts&#x2F;0 上\nStarting resourcemanager\n上一次登录：四 6月  2 15:46:34 CST 2022pts&#x2F;0 上\nStarting nodemanagers\n上一次登录：四 6月  2 15:46:40 CST 2022pts&#x2F;0 上\n[root@bd01 hadoop]# xcall jps\n----------bd01---------\n2518 SecondaryNameNode\n2215 NameNode\n2777 ResourceManager\n3118 Jps\n----------bd02---------\n1410 NodeManager\n1332 DataNode\n1564 Jps\n----------bd03---------\n1329 DataNode\n1563 Jps\n1406 NodeManager</code></pre>\n\n<h4 id=\"9、初始化Hive的Metastore\"><a href=\"#9、初始化Hive的Metastore\" class=\"headerlink\" title=\"9、初始化Hive的Metastore\"></a>9、初始化Hive的Metastore</h4><pre class=\"line-numbers language-shell\" data-language=\"shell\"><code class=\"language-shell\">[root@bd04 bin]# schematool -dbType mysql -initSchema</code></pre>\n\n<p>如果发现如下报错</p>\n<pre class=\"line-numbers language-java\" data-language=\"java\"><code class=\"language-java\">Caused by: com.ctc.wstx.exc.WstxParsingException: Illegal character entity: expansion character (code 0x8\n at [row,col,system-id]: [3215,96,&quot;file:&#x2F;opt&#x2F;moudle&#x2F;apache-hive-3.1.2-bin&#x2F;conf&#x2F;hive-site.xml&quot;]\n        at com.ctc.wstx.sr.StreamScanner.constructWfcException(StreamScanner.java:621)\n        at com.ctc.wstx.sr.StreamScanner.throwParseError(StreamScanner.java:491)\n        at com.ctc.wstx.sr.StreamScanner.reportIllegalChar(StreamScanner.java:2456)\n        at com.ctc.wstx.sr.StreamScanner.validateChar(StreamScanner.java:2403)\n        at com.ctc.wstx.sr.StreamScanner.resolveCharEnt(StreamScanner.java:2369)\n        at com.ctc.wstx.sr.StreamScanner.fullyResolveEntity(StreamScanner.java:1515)\n        at com.ctc.wstx.sr.BasicStreamReader.nextFromTree(BasicStreamReader.java:2828)\n        at com.ctc.wstx.sr.BasicStreamReader.next(BasicStreamReader.java:1123)\n        at org.apache.hadoop.conf.Configuration$Parser.parseNext(Configuration.java:3277)\n        at org.apache.hadoop.conf.Configuration$Parser.parse(Configuration.java:3071)\n        at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2964)\n        ... 15 more</code></pre>\n\n<p>但是执行之后发现报错了，提示hive-site.xml文件中的第3215行内容有问题<br>其实这个是原始配置文件本身就有的问题，最直接的就是把这一行直接删掉，删除之后的效果如下：其实就是把hive.txn.xlock.iow对应的description标签内容删掉，这样就可以了</p>\n<pre class=\"line-numbers language-shell\" data-language=\"shell\"><code class=\"language-shell\">&lt;property&gt;\n    &lt;name&gt;hive.txn.xlock.iow&lt;&#x2F;name&gt;\n    &lt;value&gt;true&lt;&#x2F;value&gt;\n    &lt;description&gt;\n    &lt;&#x2F;description&gt;\n  &lt;&#x2F;property&gt;</code></pre>\n\n<p>修改后再执行初始化命令，初始化Metastore</p>\n<pre class=\"line-numbers language-shell\" data-language=\"shell\"><code class=\"language-shell\">[root@bd04 bin]# schematool -dbType mysql -initSchema\n#出现下面则说明初始化成功\nInitialization script completed\nschemaTool completed</code></pre>\n\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>这样Hive就安装好了，注意了，目前针对Hive不需要启动任何进程</p></blockquote>\n","feature":true,"text":" Hive是建立在Hadoop上的数据仓库基础构架。它提供了一系列的工具，可以用来进行数据提取转化加载，可以简称为ETL。 什么是Hive Hive 定义了简单的类SQL查询语言，称为HQL，它允许熟悉SQL的用户直接查询Hadoop中的数据，同时，这个语言也允许熟悉MapRed...","link":"","photos":[],"count_time":{"symbolsCount":"16k","symbolsTime":"15 mins."},"categories":[{"name":"Hive","slug":"Hive","count":1,"path":"api/categories/Hive.json"}],"tags":[{"name":"Hive","slug":"Hive","count":1,"path":"api/tags/Hive.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E4%BB%80%E4%B9%88%E6%98%AFHive\"><span class=\"toc-text\">什么是Hive</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#Hive%E7%9A%84%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8\"><span class=\"toc-text\">Hive的数据存储</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#Hive%E7%9A%84%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84\"><span class=\"toc-text\">Hive的系统架构</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%A4%A7%E6%95%B0%E6%8D%AE%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E\"><span class=\"toc-text\">大数据计算引擎</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#1%E3%80%81MapReduce\"><span class=\"toc-text\">1、MapReduce</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#2%E3%80%81Tez\"><span class=\"toc-text\">2、Tez</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#3%E3%80%81Spark\"><span class=\"toc-text\">3、Spark</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#4%E3%80%81Flink\"><span class=\"toc-text\">4、Flink</span></a></li></ol></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#Metastore\"><span class=\"toc-text\">Metastore</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E5%92%8C%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E5%8C%BA%E5%88%AB\"><span class=\"toc-text\">数据仓库和数据库的区别</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#Hive-VS-Mysql\"><span class=\"toc-text\">Hive VS Mysql</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93-VS-%E6%95%B0%E6%8D%AE%E5%BA%93\"><span class=\"toc-text\">数据仓库 VS 数据库</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#OLTP-VS-OLAP\"><span class=\"toc-text\">OLTP VS OLAP</span></a></li></ol></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#Hive%E7%9A%84%E5%AE%89%E8%A3%85%E5%92%8C%E9%83%A8%E7%BD%B2\"><span class=\"toc-text\">Hive的安装和部署</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#1%E3%80%81%E4%B8%8B%E8%BD%BDHive%E5%AE%89%E8%A3%85%E5%8C%85\"><span class=\"toc-text\">1、下载Hive安装包</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#2%E3%80%81%E5%85%8B%E9%9A%86bd04%E5%AE%89%E8%A3%85hive%E5%B9%B6%E9%85%8D%E7%BD%AE%E4%B8%BAhadoop%E9%9B%86%E7%BE%A4%E5%AE%A2%E6%88%B7%E7%AB%AF\"><span class=\"toc-text\">2、克隆bd04安装hive并配置为hadoop集群客户端</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#3%E3%80%81%E5%B0%86Hive%E4%B8%8A%E4%BC%A0%E5%88%B0bd04%E8%99%9A%E6%8B%9F%E6%9C%BA%EF%BC%8C%E5%B9%B6%E8%A7%A3%E5%8E%8B\"><span class=\"toc-text\">3、将Hive上传到bd04虚拟机，并解压</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#4%E3%80%81%E6%8B%B7%E8%B4%9D%E5%B9%B6%E9%87%8D%E5%91%BD%E5%90%8D%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6\"><span class=\"toc-text\">4、拷贝并重命名配置文件</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#5%E3%80%81Centos%E5%AE%89%E8%A3%85MySQL8-0-16\"><span class=\"toc-text\">5、Centos安装MySQL8.0.16</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#6%E3%80%81%E4%BF%AE%E6%94%B9%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6\"><span class=\"toc-text\">6、修改配置文件</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#7%E3%80%81%E4%BF%AE%E6%94%B9core-site-xml%E7%9A%84%E9%85%8D%E7%BD%AE\"><span class=\"toc-text\">7、修改core-site.xml的配置</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#8%E3%80%81%E5%90%AF%E5%8A%A8Hadoop%E9%9B%86%E7%BE%A4\"><span class=\"toc-text\">8、启动Hadoop集群</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#9%E3%80%81%E5%88%9D%E5%A7%8B%E5%8C%96Hive%E7%9A%84Metastore\"><span class=\"toc-text\">9、初始化Hive的Metastore</span></a></li></ol></li></ol>","author":{"name":"张春博","slug":"blog-author","avatar":"/images/QD20000163.jpg","link":"/","description":"研发及交付中心<br>零售开发组<br>手机银行","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"mapped":true,"prev_post":{},"next_post":{"title":"Hadoop搭建简易分布式集群","uid":"3e4c7b56240531255ad11affc0c50a94","slug":"Hadoop搭建简易分布式集群","date":"2022-05-31T07:14:45.000Z","updated":"2023-03-01T07:02:24.077Z","comments":true,"path":"api/articles/Hadoop搭建简易分布式集群.json","keywords":null,"cover":"/post/Hadoop搭建简易分布式集群/cover.jpeg","text":"使用三台机器创建Hadoop简易主从分布式集群，实现一个一主两从的Hadoop集群 Hadoop简易分布式集群搭建 1. 环境准备规划三个虚拟机节点：bd01、bd02、bd03 bd01 192.168.126.201 bd02 192.168.126.202 bd03 192...","link":"","photos":[],"count_time":{"symbolsCount":"11k","symbolsTime":"10 mins."},"categories":[{"name":"Hadoop","slug":"Hadoop","count":2,"path":"api/categories/Hadoop.json"}],"tags":[{"name":"Hadoop","slug":"Hadoop","count":2,"path":"api/tags/Hadoop.json"}],"author":{"name":"张春博","slug":"blog-author","avatar":"/images/QD20000163.jpg","link":"/","description":"研发及交付中心<br>零售开发组<br>手机银行","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"feature":true}}